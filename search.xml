<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Posix多线程之旅——生产者-消费者]]></title>
    <url>%2F2018%2F04%2F03%2Fposix-producer-consumer-problem%2F</url>
    <content type="text"><![CDATA[生产者消费者问题（英语：Producer-consumer problem），也称有限缓冲问题（英语：Bounded-buffer problem），是一个多线程同步问题的经典案例。该问题描述了共享固定大小缓冲区的两个线程——即所谓的“生产者”和“消费者”——在实际运行时会发生的问题。生产者的主要作用是生成一定量的数据放到缓冲区中，然后重复此过程。与此同时，消费者也在缓冲区消耗这些数据。该问题的关键就是要保证生产者不会在缓冲区满时加入数据，消费者也不会在缓冲区中空时消耗数据。 但是在 leveldb 中有这么一段代码，很好的写出了一个后台线程传入事件异步并发执行执行。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#include &lt;iostream&gt;#include &lt;deque&gt;#include &lt;unistd.h&gt;class PosixEnv&#123;public: PosixEnv() : started_bgthread_(false) &#123; PthreadCall("mutex_init", pthread_mutex_init(&amp;mu_, NULL)); PthreadCall("cvar_init", pthread_cond_init(&amp;bgsignal_, NULL)); &#125; void Schedule(void(*function)(void*), void* arg) &#123; PthreadCall("lock", pthread_mutex_lock(&amp;mu_)); // Start background thread if necessary if (!started_bgthread_) &#123; started_bgthread_ = true; PthreadCall( "create thread", pthread_create(&amp;bgthread_, NULL, &amp;PosixEnv::BGThreadWrapper, this)); &#125; if (queue_.empty()) &#123; // 如果当前队列为空，那么后台线程此时可能为信号等待状态，激活一个等待该条件的线程 PthreadCall("signal", pthread_cond_signal(&amp;bgsignal_)); &#125; BGItem item = BGItem(); item.arg = arg; item.function = function; queue_.push_back(item); // 当 unlock 后，pthread_cond_wait 获取到条件信号和锁，就会执行之后的代码 PthreadCall("unlock", pthread_mutex_unlock(&amp;mu_)); &#125; static void* BGThreadWrapper(void* arg) &#123; reinterpret_cast&lt;PosixEnv*&gt;(arg)-&gt;BGThread(); return NULL; &#125;#pragma clang diagnostic push#pragma clang diagnostic ignored "-Wmissing-noreturn" void* BGThread() &#123; while (true) &#123; // Wait until there is an item that is ready to run PthreadCall("lock", pthread_mutex_lock(&amp;mu_));// 这个mutex主要是用来保证 pthread_cond_wait的并发性 while (queue_.empty()) &#123; // 等待条件信号，并将锁交出去，当获取到信号量 PthreadCall("wait", pthread_cond_wait(&amp;bgsignal_, &amp;mu_)); // pthread_cond_wait 会先解除之前的 pthread_mutex_lock 锁定的 mtx，然后阻塞在等待对列里休眠， // 直到再次被唤醒（大多数情况下是等待的条件成立而被唤醒，唤醒后，该进程会先锁定先 pthread_mutex_lock(&amp;mtx); &#125; void (*function)(void*) = queue_.front().function; void* arg = queue_.front().arg; queue_.pop_front(); PthreadCall("unlock", pthread_mutex_unlock(&amp;mu_)); // 异步并行执行方法，如果将这句代码放在 unlock 之前那么就是异步串行了 (*function)(arg); &#125; &#125;#pragma clang diagnostic popprivate: void PthreadCall(const char* label, int result) &#123; std::cout &lt;&lt; label &lt;&lt; std::endl; if (result != 0) &#123; fprintf(stderr, "pthread %s: %s\n", label, strerror(result)); abort(); &#125; &#125; pthread_mutex_t mu_; pthread_cond_t bgsignal_; pthread_t bgthread_; bool started_bgthread_; struct BGItem &#123; void* arg; void (*function)(void*); &#125;; typedef std::deque&lt;BGItem&gt; BGQueue; BGQueue queue_;&#125;;void speak(void* pstr)&#123; std::string* str = reinterpret_cast&lt;std::string*&gt;(pstr); printf("我要说话啦：%s\n", str-&gt;c_str());&#125;int main(int argc, const char * argv[]) &#123; std::string str("哇哈哈哈啊哈哈哈啊哈哈"); std::string str2("你的豆腐的的地方水电费违反诶我去翁"); PosixEnv env = PosixEnv(); env.Schedule(&amp;speak, &amp;str); sleep(10); env.Schedule(&amp;speak, &amp;str2); sleep(5); return 0;&#125; 上面例子，把所有的事件模型包装成一个struct BGItem 的结构体，而通过 BGQueue 对消息队列进行处理，这里没有用到延时执行的消息队列，也没有优先队列的处理，只是一个简单的生产者——消费者的模型。通过创建一个工作线程 bgthread_ 来开启线程运作，通过信号量 pthread_cond_wait 和 pthread_cond_signal 来进行控制工作线程的阻塞和运作。该例子中，主线程负责生成事件模型，而工作线程负责消费处理事件模型，而消息队列，就是我们所说的缓冲区了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Unix 常用工具命令笔记]]></title>
    <url>%2F2018%2F04%2F03%2FUnix-common-commandline%2F</url>
    <content type="text"><![CDATA[一些Unix/Linux 和 Mac/iOS 的命令行工具，用于学习、测试 iOS 开发和后端开发的常用命令，持续更新中… gnu套装工具gcc 编译器生成目标文件 （二进制机器代码）1gcc -c 生成可执行文件 （链接器重定位）1gcc -o 打开gcc所有警告1gcc -Wall ld 链接器将多个目标文件合并成一个目标文件1ld -r cat (文本文件查看和连接工具)一次显示整个文件1cat 文件名 创建一个文件1cat &gt; 文件名 将几个文件合并成一个文件1cat 文件名1 文件名2 &gt; 文件名3 更多的使用参考 ar (压缩工具——用于静态库)查看静态库文件包含了那些目标文件1ar -t libc.a 解压静态库文件，得到对目标文件进行编号和索引的 __.SYMDEF文件1ar -x libc.a 将目标文件打包成静态链接库1ar crv libmylib.a my_print.o my_match.o 删除静态链接库中的某个目标文件1ar -d lib.a my_print.o 要替换或添加新成员到库中1ar -v -r lib.a strlen.o strcat.o 1-v : 将建立新库的详细的逐个文件的描述写至标准输出。当和 -t 标志一起使用时，它给出类似于 ls -l 命令给出的长列表。当和 -x 标志一起使用时，它在每个文件前加一个名称。当和 -h 标志一起使用，它列出成员名称和更新的修改时间。 grep (文本搜索工具)搜索 services 文件内的 telnet 字段1grep telnet /etc/services netstat (内核中访问网络及相关信息的程序)查看本机的路由表1netstat -nr 显示所有socket，包括正在监听的1netstat -a 显示协议名查看某协议使用情况 netstat -p 协议名1netstat -p tcp arp用于查看高速缓存中的所有项目1arp -a 或1arp -g dns 查看DNS服务器地址 这个文件内存放DNS服务器的IP地址1cat /etc/resolv.conf 其中的两个IP地址分别是首选DNS服务器地址和备选DNS服务器地址。文件中的注释语句“Generated by Network Manager”告诉我们，这两个DNS服务器地址是由网络管理程序写入的。 将查询传递给DNS服务器，并显示返回的结果1host baidu.com tcpdump lipo 因为苹果在 mac 上使用的静态库都是一些通过多个不同架构的静态库合成的压缩文件格式，所有有了 lipo 工具来操作这个压缩文件 查看 mac os系统中静态库中包含了那些架构1lipo -info lib.a 解压出指定架构的静态库1lipo lib.a -thin armv7 -output lib-armv7.a 合并模拟器库文件和真机库文件1lipo -create -output lib.a lib-armv6.a lib-i386.a dwarfdump (用于查看 dSYM 符号集合文件) * 为文件名 查看 .dSYM 符号文件中的所有符号1dwarfdump SocialDevApp.app.dSYM &gt; text 查看地址为 0x0024d6a5 所对应的符号1dwarfdump --lookup=0x0024d6a5 SocialDevApp.app.dSYM : 查看 .dSYM 文件 UUID 是否跟 .crash 文件是否一致1dwarfdump --uuid /*.app.dSYM symbolicatecrash 符号化 .crash 文件首先要找到 symbolicatec 这个 shell 在哪里，进入 xcode.app, 使用如下命令1find . -name symbolicatecrash 符号化 .crash 1./symbolicatecrash /*.crash /*.app.dSYM &gt; /*.crash 如果发生错误1&quot;DEVELOPER_DIR&quot; is not defined at ./symbolicatecrash line 69. 使用如下1export DEVELOPER_DIR=/Applications/Xcode.app/Contents/Developer xcode 中所有 dSYM 文件的位置1~/Library/Developer/Xcode/Archives objdump 用于查看 目标文件(.o) 或 可 执行的目标文件(.out) 构成的 GCC 工具 反汇编目标文件1objdump -d main.o &gt; text 显示符号表入口1objdump -t main.o 尽可能反汇编出源代码1objdump -S main.o file用于查看文件编码的命令 12345678file info.plist输出：info.plist: Apple binary property listfile SsjjCoreSdk输出：SsjjCoreSdk: Mach-O universal binary with 2 architectures: [arm_v7: current ar archive] [arm64]SsjjCoreSdk (for architecture armv7): current ar archiveSsjjCoreSdk (for architecture arm64): current ar archive lsof (一切皆文件) lsof（list open files）是一个查看当前系统文件的工具。在Unix 环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，该文件描述符提供了大量关于这个应用程序本身的信息。 查看系统中端口占用情况1lsof -i 查看某一端口的占用情况：lsof -i:端口号1lsof -i:21 查找某个文件相关的进程：lsof 文件名1lsof /bin/bash 更多使用 ps (查看进程)显示所有进程信息1ps -A ps 与grep 组合使用，查找特定进程1ps -ef|grep ssh 列出目前所有的正在内存中的程序1ps aux otool (Mac上的objdump)反汇编 mach-o 目标文件1otool -tV main.out]]></content>
      <tags>
        <tag>Unix/Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出 GCD 线程使用]]></title>
    <url>%2F2018%2F04%2F02%2Fgrand-central-dispatch-introduction%2F</url>
    <content type="text"><![CDATA[串行与并行同步和异步针对的是线程队列，所谓的线程队列可以理解为一组线程的数组。 串行队列：队列中是事件有序执行，遵循 FIFO（first in first out）的原则，先进入队列的事件先执行。 串行队列创建：123dispatch_queue_t queue = dispatch_queue_create("com.queue.serial", DISPATCH_QUEUE_SERIAL);dispatch_get_main_queue() // 主队列，也是串行队列 并行队列并行队列中的事件在逻辑上是一起执行的，但是这是要根据机器 CPU 的情况而定，在 C++ 线程库中，std::thread::hardware_concurrency() 能获取到当前机器最大能并发的线程数量，iPhone6P 中为 2，也就是说最大同时能处理两个并发线程任务，其他后面添加的任务都得等待两个任务中的其中一个执行完了，才可以执行。 123dispatch_queue_t queue = dispatch_queue_create("com.queue.concurrent", DISPATCH_QUEUE_CONCURRENT);dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0); // 全局并发队列 同步和异步同步和异步针对的是线程，那么什么是同步线程，什么是异步线程。 同步线程：阻塞当前线程，要等待同步线程内的任务执行完了并且返回以后，才可以继续执行被阻塞线程的事件。 同步线程创建：1dispatch_sync(queue, block); 异步线程：不阻塞当前线程，等当前线程完成时间片（完成当前事件）切换后再执行异步线程。 异步线程创建：1dispatch_async(queue, block); 线程问题主线程中的死锁12345NSLog(@&quot;1&quot;);dispatch_sync(dispatch_get_main_queue(), ^()&#123; NSLog(@&quot;2&quot;);&#125;);NSLog(@&quot;3&quot;); 输出：1 如果上面代码是在主线程当中执行的，那么就会造成我们的死锁问题，注意是主线程当中，后面我们还有一个测试说明。假定上面代码为主线程中执行的代码，如果不造成死锁的情况是输出应该是 1，2，3，但现在事件只执行了 1，那么死锁就很明显了，我们现在对它进行分析。 dispatch_sync 同步线程，将当前线程阻塞，先执行block（@”2”) 然后解放线程dispatch_get_main_queue 主线程队列，也可以叫做串行队列，将 dispatch_sync 同步线程放到队列后，先执行 ( @”3”) 再执行同步线程，遵循 FIFO 的原则。当时因为 dispatch_sync 是在主线程创建的，所以主线程被阻塞，主线程的事件(@”3”) 要等待 dispatch_sync 的 block 执行完后才能执行所以事件(@”3”)无法执行，事件(@”2”)更无法执行，相互等待造成死锁。 dispatch_sync(dispatch_get_main_queue(), block)是否一定会造成死锁呢？上面问题如果并不是放在主线程中有会怎么样？ 12345678910NSLog(@&quot;1&quot;);dispatch_queue_t queue = dispatch_queue_create(&quot;com.queue.concurrent&quot;, DISPATCH_QUEUE_CONCURRENT);dispatch_async(queue), ^()&#123; NSLog(@&quot;2&quot;); dispatch_sync(dispatch_get_main_queue(), ^()&#123; NSLog(@&quot;3&quot;); &#125;); NSLog(@&quot;4&quot;);&#125;);NSLog(@&quot;5&quot;); 输出: 1，5，2，3，4 输出中，可以看得出所有事件全部都执行完成，没有造成死锁，但是明明使用了 dispatch_sync(dispatch_get_main_queue(), block);这个经常被说成会造成死锁的方法，但是为什么这里没有造成死锁呢，我们来分析一下。 dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT,block) 中， dispatch_async 异步线程，将其放在了 dispatch_get_global_queue 全局队列，也可以叫并行队列中，主线程不用等待异步 dispatch_async 内的事件（block）执行完成，所以直接执行了事件(@“1”)和事件(@”5”)。当线程时间片切换出来，异步线程内的事件(block)便开始执行了，所以事件(@”2”) 便执行了。当运行到 dispatch_sync(dispatch_get_main_queue(),block) 中，dispatch_sync 阻塞当前线程，细想一下，当前线程是一个异步线程并不是主线程，事件(@”4”)又是在这个异步线程中的事件，所以要等待 dispatch_sync 同步线程内的事件执行完了，才可以执行。同步线程放在 dispatch_get_main_queue 主线程队列中，主线程队列同时也是一个串行队列，所以事件(@”3”) 一定会在事件@(“1”)和事件(@”5”)之后，当执行完事件(@”3”)便可以执行事件(@”4”)了。 上面例子说明一件事，dispatch_async 同步线程会阻塞当前线程直至同步线程内的事件(block)执行完，至于是否会发生死锁，就得看同步线程所阻塞的线程是否存在它的线程队列（queue）中。 123current threaddispatch_sync(queue), block) 第一个例子中，current thread 为主线程，queue 主线程队列，主线程属于主线程队列，所以造成死锁。 第二个例子中，current thread 为我们所开启的异步线程 dispatch_async，并且放在我们自己所创建的 dispatch_queue_t queue = dispatch_queue_create(&quot;com.queue.concurrent&quot;, DISPATCH_QUEUE_CONCURRENT); 异步线程队列中，queue 为主线程队列，异步线程 dispatch_async 并不属于主线程队列中，所以并没有造成死锁。 异步串行队列和同步串行队列首先我们做一个比较，在串行队列中开启一个异步线程，然后再异步线程的事件中再开启一个同步线程。（默认下面例子都是在主线程中运行） 123456789101112dispatch_queue_t queue = dispatch_queue_create("com.queue.CONCURRENT", DISPATCH_QUEUE_CONCURRENT);NSLog(@"1");dispatch_async(queue, ^() &#123; NSLog(@"2"); dispatch_sync(queue, ^()&#123; NSLog(@"3"); &#125;); NSLog(@"4");&#125;);NSLog(@"5"); 输出：1，5，2，3，4 然后将 queue 换成一个串行队列，看看效果如何 123456789101112dispatch_queue_t queue2 = dispatch_queue_create("com.queue.SERIAL", DISPATCH_QUEUE_SERIAL);NSLog(@"1");dispatch_async(queue2, ^() &#123; NSLog(@"2"); dispatch_sync(queue2, ^()&#123; NSLog(@"3"); &#125;); NSLog(@"4");&#125;);NSLog(@"5"); 输出：1，5，2 第一个例子使用 DISPATCH_QUEUE_CONCURRENT 并发队列，输出正常，而第二个例子中使用了 DISPATCH_QUEUE_SERIAL 串行队列，发生了死锁，后面的事件 (@”3”) 和事件 (@”4”)便无法执行。 我们首先分析一下第一个例子，为什么并没有发生死锁，首先我们往并发队列 queue 中添加了dispatch_async 异步线程 ，主线程并不等待异步线程的执行，所以事件 (@”1”) 后便马上执行事件 (@”5”)，当内核线程空闲，加载并发队列 queue 中的 dispatch_async 异步线程 并执行线程中的事件(block) 的，事件 (@”2”) 马上就会被执行。当遇到了 dispatch_sync 同步线程的时候，当前线程，也就是 dispatch_async 这个异步线程会进入阻塞，等待 dispatch_sync 同步线程内的事件(block) 执行完，才可以往下执行事件(@”4”)，我们并将dispatch_sync 同步线程放进了 queue 并发队列当中去，并发队列的特点就是逻辑上是一起执行的，所以 dispatch_sync 同步线程加入 queue 后就马上被执行了，当事件(@”3”)执行完后并且返回，阻塞放开，事件(@”4”)并马上被执行。全过程并没有发生死锁。 我们再来看看第二个例子，首先我们往串行队列 queue 中添加了dispatch_async 异步线程 ，其后过程跟第一个例子一样，直到遇到了 dispatch_sync(queue2, block) ，dispatch_sync` 同步线程 阻塞了 dispatch_async 异步线程，并将同步线程放进了 queue2 串行队列中，串行队列的特别是遵循 FIFO 特点，要必先执行完 dispatch_async 异步线程的事件(block)，才能执行同步线程 dispatch_sync 的事件 (block)，所以造成了死锁。 AFNetWorking 怎么使用同步线程12345678910111213141516171819202122self.synchronizationQueue = dispatch_queue_create([name cStringUsingEncoding:NSASCIIStringEncoding], DISPATCH_QUEUE_SERIAL);- (nullable AFImageDownloadReceipt *)downloadImageForURLRequest:(NSURLRequest *)request withReceiptID:(nonnull NSUUID *)receiptID success:(nullable void (^)(NSURLRequest *request, NSHTTPURLResponse * _Nullable response, UIImage *responseObject))success failure:(nullable void (^)(NSURLRequest *request, NSHTTPURLResponse * _Nullable response, NSError *error))failure &#123; dispatch_sync(self.synchronizationQueue, ^&#123; NSString *URLIdentifier = request.URL.absoluteString; if (URLIdentifier == nil) &#123; if (failure) &#123; NSError *error; dispatch_async(dispatch_get_main_queue(), ^&#123; failure(request, nil, error); &#125;); &#125; return; &#125; ... &#125;);&#125; 上面一段代码才子 AFNetWorking 中的 AFImageDownloader.m 文件当中，作者创建了 synchronizationQueue 串行队列专门用作阻塞当前线程，限制性同步队列中的事件，判断 url 是否为空，但是为什么要这样做呢？ 原因1：因为对象方法 downloadImageForURLRequest:withReceiptID:success:failure 是同一个对象在多个异步线程的并发队列当中执行的，因为并发在逻辑上会同时触发异步线程，那么传进来的参数（request，receiptID，success，failure）会由于资源竞争(condition race) 的情况下会被覆盖，所以我们需要进行阻塞这个线程，先执行完一个请求后再执行另外一个请求 但是会有人问：为什么么不用 @synchronized (&lt;#lock#&gt;) {} ?因为我们首先不确定调用对象方法 downloadImageForURLRequest:withReceiptID:success:failure 是否必定在异步线程中被调用，莫名的加锁会消耗资源，当我们使用了 dispatch_sync(self.synchronizationQueue,block) 后，如果主线程当中被调用，也只会忽视这个方法，直接调用 block，因为阻塞主线程，往并不是主线程队列的线程队列中添加时间，是没有意义的。 使用 dispatch_sync(self.synchronizationQueue,block) 需要注意什么问题？其实上面这么写，是有问题的，当方法 downloadImageForURLRequest:withReceiptID:success:failure 的调用上层，也是dispatch_sync(self.synchronizationQueue,block) 的情况下，就会造成死锁，就像下面一样 1234567dispatch_sync(self.synchronizationQueue, ^()&#123; NSLog(@&quot;2&quot;); dispatch_sync(self.synchronizationQueue, ^()&#123; NSLog(@&quot;3&quot;); &#125;); NSLog(@&quot;4&quot;);&#125;); 或1234567dispatch_async(self.synchronizationQueue, ^()&#123; NSLog(@&quot;2&quot;); dispatch_sync(self.synchronizationQueue, ^()&#123; NSLog(@&quot;3&quot;); &#125;); NSLog(@&quot;4&quot;);&#125;); 至于怎么分析，为什么会发生死锁，各位看官，这就留给你们的作业，看了这么多，相信大家也会明白，特别是第二个例子，我们刚讲过，希望大家能在这篇博客中学到东西。 写在最后： 为什么要写这篇文章呢？主要今天在某公司面试的时候，被问到了关于 GCD 的线程问题，在我说出来答案后，面试官依然坚持已见，认为我是错的，写这篇博客的目的在于，不管这个面试官是否会游览博客，也让更多的面试官可以好好更新自己的知识储备库，不要做井底之蛙。其实在我看来，面试是一个双向交流的过程，我并不在意是否能你们公司工作，毕竟我也不想同事是一群无法交流的人，一个开心愉快并且能够助我成长的工作环境才是我真正需要的。]]></content>
      <tags>
        <tag>iOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机程序编译过程]]></title>
    <url>%2F2018%2F04%2F02%2Fprogram-compile-introduction%2F</url>
    <content type="text"><![CDATA[现在这个社会充斥着太多的水货程序员了，他们不懂任何计算机原理，但是他们依旧做着公司的业务，很好的完成老板交代的任务，但是这些东西永远对他们来说都说一个黑盒子，他们不会知道为什么会产生段错误，为什么会有悬挂指针，不知道为什么会产生链接错误，什么是缺失符号，更不知道为什么函数 return 一个局部变量就会段错误，返回一个字面量常量就不会，当发生这些问题来只能谷歌，stackoverflow，甚至只能是百度。所以，这里一步一步给大家科普，不断普及一些“常识”让大家更好的理解我们编写的软件程序。 计算机程序编译过程分为4个步骤： 预处理 编译 汇编 链接 预处理1$gcc -E hello.c -o hello.i 或1$cpp hello.c &gt; hello.i 预编译过程主要： 处理预编译指令，例如 #define，#if，#ifdefine 等。 将 #include 包含文件插入到该预编译指令的位置 删除所有的注析 // 、/**/ 编译1$gcc –S hello.i –o hello.s 或1$gcc –S hello.c –o hello.s 编译代表了一整个过程： 词法分析 语法分析 语义分析 源代码优化 代码生成 目标代码优化 词法分析扫描字节序并产生记号。词法分析产生的记号一般可以分为如下几类：关键字、标识符、字面量（包含数字、字符串等）和特殊符号（如加号、等号）。 语法分析语法分析器（Grammar Parser）将对由扫描器产生的记号进行语法分析，从而产生语法树（Syntax Tree）。由语法分析器生成的语法树就是以表达式（Expression）为节点的树。 语义分析语法分析仅仅是完成了对表达式的语法层面的分析，但是它并不了解这个语句是否真正有意义。编译器所能分析的语义是静态语义（Static Semantic），所谓静态语义是指在编译期可以确定的语义，与之对应的动态语义（Dynamic Semantic）就是只有在运行期才能确定的语义。静态语义通常包括声明和类型的匹配，类型的转换。 动态语义和静态语义? 比如将一个浮点型赋值给一个指针的时候，语义分析程序会发现这个类型不匹配，编译器将会报错。动态语义一般指在运行期出现的语义相关的问题，比如将0作为除数是一个运行期语义错误。 中间语言生成中间代码使得编译器可以被分为前端和后端。编译器前端负责产生机器无关的中间代码，编译器后端将中间代码转换成目标机器代码。这样对于一些可以跨平台的编译器而言，它们可以针对不同的平台使用同一个前端和针对不同机器平台的数个后端。 目标代码生成与优化 代码级优化器产生中间代码标志着下面的过程都属于编译器后端。编译器后端主要包括代码生成器（Code Generator）和目标代码优化器（Target Code Optimizer）。 代码生成器将中间代码转换成目标机器代码，这个过程十分依赖于目标机器。 对于上面例子中的中间代码，代码生成器可能会生成下面的代码序列12345movl index, %ecx ; value of index to ecxaddl $4, %ecx ; ecx = ecx + 4mull $8, %ecx ; ecx = ecx * 8movl index, %eax ; value of index to eaxmovl %ecx, array(,eax,4) ; array[index] = ecx 汇编1$as hello.s –o hello.o 或1$gcc –c hello.c –o hello.o 汇编器(as)将汇编代码翻译成机器语言指令，把这些指令打包成一种叫做可重定位目标程序(relocatable)的格式，并将结果保持在目标文件 hello.o 中。hello.o 是一个二进制文件。 链接1$ld -static /usr/lib/crt1.o /usr/lib/crti.o /usr/lib/gcc/i486-linux-gnu/4.1.3/crtbeginT.o -L/usr/lib/gcc/i486-linux-gnu/4.1.3 -L/usr/lib -L/lib hello.o --start-group -lgcc -lgcc_eh -lc --end-group /usr/lib/gcc/i486-linux-gnu/4.1.3/crtend.o /usr/lib/crtn.o 链接阶段最重要的工作就是重定位，将所有的目标文件都链接起来，在 #include 文件中的函数声明原本只会生成一个没有跳转地址的指令，重定位的工作在其他目标文件中找到这些目标地址，并把地址填补进去。 链接分为静态链接和动态链接，也就是我们通常说的私有对象和共享对象。这里得展开另外一篇来讲了，内容太多。 链接就像将所有的组件合成一起，组成一个整体。]]></content>
      <tags>
        <tag>计算机基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++项目中遇到的一些方法归纳(持续更新)]]></title>
    <url>%2F2018%2F04%2F02%2Fcpp-project-new-learning%2F</url>
    <content type="text"><![CDATA[浏览 C++ 项目中收集到的代码学习片段 将 string 类型，转换为模板类型123456template &lt;typename P0&gt; bool FromString(const std::string&amp; s, P0* p) &#123; std::istringstream iss(s); iss &gt;&gt; std::boolalpha &gt;&gt; *p; return !iss.fail(); &#125; 将模板类型 ，转换为string 类型12345678template &lt;class T&gt;static bool ToString(const T &amp;t, std::string* s) &#123; RTC_DCHECK(s); std::ostringstream oss; oss &lt;&lt; std::boolalpha &lt;&lt; t; *s = oss.str(); return !oss.fail();&#125; Mach内核获得Cpu使用率百分比NSInteger ARDGetCpuUsagePercentage() { // Create an array of thread ports for the current task. const task_t task = mach_task_self(); thread_act_array_t thread_array; mach_msg_type_number_t thread_count; if (task_threads(task, &amp;thread_array, &amp;thread_count) != KERN_SUCCESS) { return -1; } // Sum cpu usage from all threads. float cpu_usage_percentage = 0; thread_basic_info_data_t thread_info_data = {}; mach_msg_type_number_t thread_info_count; for (size_t i = 0; i &lt; thread_count; ++i) { thread_info_count = THREAD_BASIC_INFO_COUNT; kern_return_t ret = thread_info(thread_array[i], THREAD_BASIC_INFO, (thread_info_t)&amp;thread_info_data, &amp;thread_info_count); if (ret == KERN_SUCCESS) { cpu_usage_percentage += 100.f * (float)thread_info_data.cpu_usage / TH_USAGE_SCALE; } } // Dealloc the created array. vm_deallocate(task, (vm_address_t)thread_array, sizeof(thread_act_t) * thread_count); return lroundf(cpu_usage_percentage);]]></content>
      <tags>
        <tag>C++, 代码片段</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异步iO的演变历程]]></title>
    <url>%2F2018%2F04%2F02%2Fasynchronous-io-introduction%2F</url>
    <content type="text"><![CDATA[英文原文Most beginning programmers start with blocking IO calls. An IO call is synchronous if, when you call it, it does not return until the operation is completed, or until enough time has passed that your network stack gives up. When you call “connect()” on a TCP connection, for example, your operating system queues a SYN packet to the host on the other side of the TCP connection. It does not return control back to your application until either it has received a SYN ACK packet from the opposite host, or until enough time has passed that it decides to give up. 大多数编程都是从阻塞IO开始的，一个 IO 的调用是同步的，当你调用它时，他会一直等待到操作完成后才返回，或者等到足够时间的时候后你的网络堆栈主动放弃。当你在 TCP 连接上调用 “connect()” ，例如你操作系统发送 SYN 数据包到 TCP 连接的另一端主机的时。它不会立即将控制权返回给你的应用程序，而直到它收到来自对方主机的SYN ACK数据包，或者直到超时后主动放弃的时候，才会将控制权返回。 Here’s an example of a really simple client using blocking network calls. It opens a connection to www.google.com, sends it a simple HTTP request, and prints the response to stdout. 这是一个客户端非常简单的使用阻塞网络函数的的例子。例子中打开与www.google.com的连接，向www.google.com 发送一个简单的HTTP请求，并将响应打印到stdout Example: A simple blocking HTTP client 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/* For sockaddr_in */#include &lt;netinet/in.h&gt;/* For socket functions */#include &lt;sys/socket.h&gt;/* For gethostbyname */#include &lt;netdb.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;int main(int c, char **v)&#123; const char query[] = "GET / HTTP/1.0\r\n" "Host: www.google.com\r\n" "\r\n"; const char hostname[] = "www.google.com"; struct sockaddr_in sin; struct hostent *h; const char *cp; int fd; ssize_t n_written, remaining; char buf[1024]; /* Look up the IP address for the hostname. Watch out; this isn't threadsafe on most platforms. */ h = gethostbyname(hostname); if (!h) &#123; fprintf(stderr, "Couldn't lookup %s: %s", hostname, hstrerror(h_errno)); return 1; &#125; if (h-&gt;h_addrtype != AF_INET) &#123; fprintf(stderr, "No ipv6 support, sorry."); return 1; &#125; /* Allocate a new socket */ fd = socket(AF_INET, SOCK_STREAM, 0); if (fd &lt; 0) &#123; perror("socket"); return 1; &#125; /* Connect to the remote host. */ sin.sin_family = AF_INET; sin.sin_port = htons(80); sin.sin_addr = *(struct in_addr*)h-&gt;h_addr; if (connect(fd, (struct sockaddr*) &amp;sin, sizeof(sin))) &#123; perror("connect"); close(fd); return 1; &#125; /* Write the query. */ /* XXX Can send succeed partially? */ cp = query; remaining = strlen(query); while (remaining) &#123; n_written = send(fd, cp, remaining, 0); if (n_written &lt;= 0) &#123; perror("send"); return 1; &#125; remaining -= n_written; cp += n_written; &#125; /* Get an answer back. */ while (1) &#123; ssize_t result = recv(fd, buf, sizeof(buf), 0); if (result == 0) &#123; break; &#125; else if (result &lt; 0) &#123; perror("recv"); close(fd); return 1; &#125; fwrite(buf, 1, result, stdout); &#125; close(fd); return 0;&#125; All of the network calls in the code above are blocking: the gethostbyname does not return until it has succeeded or failed in resolving www.google.com; the connect does not return until it has connected; the recv calls do not return until they have received data or a close; and the send call does not return until it has at least flushed its output to the kernel’s write buffers. 在上面代码中所有的网络函数都是阻塞的：在解析 www.google.com 成功或失败之前，gethostbyname 函数不会返回。connect 函数直到它已经连接才会返回。recv 函数在收到数据或 close 之前不会返回;并且send 函数不会返回，直到它至少将其输出刷新到内核的写缓冲区。 Now, blocking IO is not necessarily evil. If there’s nothing else you wanted your program to do in the meantime, blocking IO will work fine for you. But suppose that you need to write a program to handle multiple connections at once. To make our example concrete: suppose that you want to read input from two connections, and you don’t know which connection will get input first. 其实阻塞 IO 不一定是一件坏事。如果你不希望同时处理某些事情的时候，那么阻塞 IO 是非常合适的。但是，假设你需要编写一个程序来同时处理多个连接，这个时候阻塞 IO 就非常不适合我们了。为了使我们的例子具体化：假设你想要从两个连接读取输入，并且你不知道哪个连接首先得到输入。 You can’t say Bad Example 1234567891011121314/* This won't work. */char buf[1024];int i, n;while (i_still_want_to_read()) &#123; for (i=0; i&lt;n_sockets; ++i) &#123; n = recv(fd[i], buf, sizeof(buf), 0); if (n==0) handle_close(fd[i]); else if (n&lt;0) handle_error(fd[i], errno); else handle_input(fd[i], buf, n); &#125;&#125; because if data arrives on fd[2] first, your program won’t even try reading from fd[2] until the reads from fd[0] and fd[1] have gotten some data and finished. 因为如果数据首先到达fd [2],直到 fd [0]和fd [1]的读取已经获得一些数据并结束为止，程序甚至不会尝试读取fd [2]。 Sometimes people solve this problem with multithreading, or with multi-process servers. One of the simplest ways to do multithreading is with a separate process (or thread) to deal with each connection. Since each connection has its own process, a blocking IO call that waits for one connection won’t make any of the other connections’ processes block. 有时人们通过多线程或者多进程的方式去解决这个问题。一个最简单使用多线程的方式是通过分离进程(或者线程)去处理每个连接。由于每个连接都有自己的进程，等待一个连接的阻塞IO调用将不会使任何其他连接的进程阻塞。 Here’s another example program. It is a trivial server that listens for TCP connections on port 40713, reads data from its input one line at a time, and writes out the ROT13 obfuscation of line each as it arrives. It uses the Unix fork() call to create a new process for each incoming connection. 这里有另外一个示例程序。这是一个微不足道的服务器，它监听者端口40713上的TCP连接，一次从它的输入中读取一行数据，并在每一行到达时写出ROT13混淆，它使用 Unix fork() 函数为每个传入的连接创建一个新的进程 Example: Forking ROT13 server123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/* For sockaddr_in */#include &lt;netinet/in.h&gt;/* For socket functions */#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_LINE 16384charrot13_char(char c)&#123; /* We don't want to use isalpha here; setting the locale would change * which characters are considered alphabetical. */ if ((c &gt;= 'a' &amp;&amp; c &lt;= 'm') || (c &gt;= 'A' &amp;&amp; c &lt;= 'M')) return c + 13; else if ((c &gt;= 'n' &amp;&amp; c &lt;= 'z') || (c &gt;= 'N' &amp;&amp; c &lt;= 'Z')) return c - 13; else return c;&#125;voidchild(int fd)&#123; char outbuf[MAX_LINE+1]; size_t outbuf_used = 0; ssize_t result; while (1) &#123; char ch; result = recv(fd, &amp;ch, 1, 0); if (result == 0) &#123; break; &#125; else if (result == -1) &#123; perror("read"); break; &#125; /* We do this test to keep the user from overflowing the buffer. */ if (outbuf_used &lt; sizeof(outbuf)) &#123; outbuf[outbuf_used++] = rot13_char(ch); &#125; if (ch == '\n') &#123; send(fd, outbuf, outbuf_used, 0); outbuf_used = 0; continue; &#125; &#125;&#125;voidrun(void)&#123; int listener; struct sockaddr_in sin; sin.sin_family = AF_INET; sin.sin_addr.s_addr = 0; sin.sin_port = htons(40713); listener = socket(AF_INET, SOCK_STREAM, 0);#ifndef WIN32 &#123; int one = 1; setsockopt(listener, SOL_SOCKET, SO_REUSEADDR, &amp;one, sizeof(one)); &#125;#endif if (bind(listener, (struct sockaddr*)&amp;sin, sizeof(sin)) &lt; 0) &#123; perror("bind"); return; &#125; if (listen(listener, 16)&lt;0) &#123; perror("listen"); return; &#125; while (1) &#123; struct sockaddr_storage ss; socklen_t slen = sizeof(ss); int fd = accept(listener, (struct sockaddr*)&amp;ss, &amp;slen); if (fd &lt; 0) &#123; perror("accept"); &#125; else &#123; if (fork() == 0) &#123; child(fd); exit(0); &#125; &#125; &#125;&#125;intmain(int c, char **v)&#123; run(); return 0;&#125; So, do we have the perfect solution for handling multiple connections at once? Can I stop writing this book and go work on something else now? Not quite. First off, process creation (and even thread creation) can be pretty expensive on some platforms. In real life, you’d want to use a thread pool instead of creating new processes. But more fundamentally, threads won’t scale as much as you’d like. If your program needs to handle thousands or tens of thousands of connections at a time, dealing with tens of thousands of threads will not be as efficient as trying to have only a few threads per CPU. 所以，我们拥有同时处理多个连接的完美解决方案了吗？我可以停止写这篇文章了吗？还不行。首先，进程的创建（还有事件线程的创建）在一些平台上可能是非常昂贵的。在现实场景当中，你更希望使用线程池而不是创建一个新的进程对象。但其实，线程的消耗并不是像你想的那么小。如果你的程序需要一次性的操作成千上万的连接，数以万计的线程将会使你的的 CPU 并不会像对待只有几个线程那样高效了。 But if threading isn’t the answer to having multiple connections, what is? In the Unix paradigm, you make your sockets nonblocking. The Unix call to do this is: 但是如果线程不是处理多连接的完美解决方案，哪最终方案到底在哪里？在Unix范例中，你可以使你的 sockets 不受阻塞。Unix函数如下： fcntl(fd, F_SETFL, O_NONBLOCK);where fd is the file descriptor for the socket.[A file descriptor is the number the kernel assigns to the socket when you open it. You use this number to make Unix calls referring to the socket.]Once you’ve made fd (the socket) nonblocking, from then on, whenever you make a network call to fd the call will either complete the operation immediately or return with a special error code to indicate “I couldn’t make any progress now, try again.” So our two-socket example might be naively written as: fd 是 socket 的文件描述符[一个文件描述符是打开它时内核分配给 socket 的编号。你可以使用该编号让Unix 函数可以引用到该 socket]一旦你使得fd( socket 套接字)不阻塞，那么无论何时你对fd进行网络函数调用（因为网络函数都是阻塞）的，它都会马上完成操作或者返回一个特别的错误代码:” couldn’t make any progress now, try again.”。所以我们的双套接字示例程序被天真的写成如下： Bad Example: busy-polling all sockets123456789101112131415161718192021/* This will work, but the performance will be unforgivably bad. */int i, n;char buf[1024];for (i=0; i &lt; n_sockets; ++i) fcntl(fd[i], F_SETFL, O_NONBLOCK);while (i_still_want_to_read()) &#123; for (i=0; i &lt; n_sockets; ++i) &#123; n = recv(fd[i], buf, sizeof(buf), 0); if (n == 0) &#123; handle_close(fd[i]); &#125; else if (n &lt; 0) &#123; if (errno == EAGAIN) ; /* The kernel didn't have any data for us to read. */ else handle_error(fd[i], errno); &#125; else &#123; handle_input(fd[i], buf, n); &#125; &#125;&#125; Now that we’re using nonblocking sockets, the code above would work… but only barely. The performance will be awful, for two reasons. First, when there is no data to read on either connection the loop will spin indefinitely, using up all your CPU cycles. Second, if you try to handle more than one or two connections with this approach you’ll do a kernel call for each one, whether it has any data for you or not. So what we need is a way to tell the kernel “wait until one of these sockets is ready to give me some data, and tell me which ones are ready.” 我们现在使用了非阻塞的 sockets，上述代码是可以运行，但仅此而已。它的性能将会非常差，有两个原因：第一，当所有的连接都没有数据读取的时候，while 和 for 操作将无限循环，耗尽了你的 CPU 周期。第二，如果你尝试用这种方法处理多于一个或两个连接，不管你是否有数据需要处理，都将为每个连接执行一次内核调用。所以，我们所需的方式是告诉内核”等待到其中一个 socket 准备好传递数据给我们的时候，才告诉我们那个socket 已经准备就绪。” The oldest solution that people still use for this problem is select(). The select() call takes three sets of fds (implemented as bit arrays): one for reading, one for writing, and one for “exceptions”. It waits until a socket from one of the sets is ready and alters the sets to contain only the sockets ready for use. Here is our example again, using select: 人们解决这个问题最常用的方法就是使用 select() 函数。使用 select() 函数解决这个问题的时候需要使用三个 fds(位数组的实现) 集合：分别处理 读、写 和 “异常” 操作。select()函数会一直等待，直到来自数组中的某一个 socket 准备就绪时，修改参数的集合并将准备就绪的 socket 放入作为该集合当中。以下使用 select 函数重新实现我们的示例: Example: Using select1234567891011121314151617181920212223242526272829303132333435/* If you only have a couple dozen fds, this version won't be awful */fd_set readset;int i, n;char buf[1024];while (i_still_want_to_read()) &#123; int maxfd = -1; FD_ZERO(&amp;readset); /* Add all of the interesting fds to readset */ for (i=0; i &lt; n_sockets; ++i) &#123; if (fd[i]&gt;maxfd) maxfd = fd[i]; FD_SET(fd[i], &amp;readset); &#125; /* Wait until one or more fds are ready to read */ select(maxfd+1, &amp;readset, NULL, NULL, NULL); /* Process all of the fds that are still set in readset */ for (i=0; i &lt; n_sockets; ++i) &#123; if (FD_ISSET(fd[i], &amp;readset)) &#123; n = recv(fd[i], buf, sizeof(buf), 0); if (n == 0) &#123; handle_close(fd[i]); &#125; else if (n &lt; 0) &#123; if (errno == EAGAIN) ; /* The kernel didn't have any data for us to read. */ else handle_error(fd[i], errno); &#125; else &#123; handle_input(fd[i], buf, n); &#125; &#125; &#125;&#125; And here’s a reimplementation of our ROT13 server, using select() this time. 这次使用了 select() 函数重新实现了我们的 ROT13 服务器。 Example: select()-based ROT13 server123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228/* For sockaddr_in */#include &lt;netinet/in.h&gt;/* For socket functions */#include &lt;sys/socket.h&gt;/* For fcntl */#include &lt;fcntl.h&gt;/* for select */#include &lt;sys/select.h&gt;#include &lt;assert.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#define MAX_LINE 16384charrot13_char(char c)&#123; /* We don't want to use isalpha here; setting the locale would change * which characters are considered alphabetical. */ if ((c &gt;= 'a' &amp;&amp; c &lt;= 'm') || (c &gt;= 'A' &amp;&amp; c &lt;= 'M')) return c + 13; else if ((c &gt;= 'n' &amp;&amp; c &lt;= 'z') || (c &gt;= 'N' &amp;&amp; c &lt;= 'Z')) return c - 13; else return c;&#125;struct fd_state &#123; char buffer[MAX_LINE]; size_t buffer_used; int writing; size_t n_written; size_t write_upto;&#125;;struct fd_state *alloc_fd_state(void)&#123; struct fd_state *state = malloc(sizeof(struct fd_state)); if (!state) return NULL; state-&gt;buffer_used = state-&gt;n_written = state-&gt;writing = state-&gt;write_upto = 0; return state;&#125;voidfree_fd_state(struct fd_state *state)&#123; free(state);&#125;voidmake_nonblocking(int fd)&#123; fcntl(fd, F_SETFL, O_NONBLOCK);&#125;intdo_read(int fd, struct fd_state *state)&#123; char buf[1024]; int i; ssize_t result; while (1) &#123; result = recv(fd, buf, sizeof(buf), 0); if (result &lt;= 0) break; for (i=0; i &lt; result; ++i) &#123; if (state-&gt;buffer_used &lt; sizeof(state-&gt;buffer)) state-&gt;buffer[state-&gt;buffer_used++] = rot13_char(buf[i]); if (buf[i] == '\n') &#123; state-&gt;writing = 1; state-&gt;write_upto = state-&gt;buffer_used; &#125; &#125; &#125; if (result == 0) &#123; return 1; &#125; else if (result &lt; 0) &#123; if (errno == EAGAIN) return 0; return -1; &#125; return 0;&#125;intdo_write(int fd, struct fd_state *state)&#123; while (state-&gt;n_written &lt; state-&gt;write_upto) &#123; ssize_t result = send(fd, state-&gt;buffer + state-&gt;n_written, state-&gt;write_upto - state-&gt;n_written, 0); if (result &lt; 0) &#123; if (errno == EAGAIN) return 0; return -1; &#125; assert(result != 0); state-&gt;n_written += result; &#125; if (state-&gt;n_written == state-&gt;buffer_used) state-&gt;n_written = state-&gt;write_upto = state-&gt;buffer_used = 0; state-&gt;writing = 0; return 0;&#125;voidrun(void)&#123; int listener; struct fd_state *state[FD_SETSIZE]; struct sockaddr_in sin; int i, maxfd; fd_set readset, writeset, exset; // 三组操作，读、写和异常 sin.sin_family = AF_INET; sin.sin_addr.s_addr = 0; sin.sin_port = htons(40713); for (i = 0; i &lt; FD_SETSIZE; ++i) state[i] = NULL; listener = socket(AF_INET, SOCK_STREAM, 0); make_nonblocking(listener);#ifndef WIN32 &#123; int one = 1; setsockopt(listener, SOL_SOCKET, SO_REUSEADDR, &amp;one, sizeof(one)); &#125;#endif if (bind(listener, (struct sockaddr*)&amp;sin, sizeof(sin)) &lt; 0) &#123; perror("bind"); return; &#125; if (listen(listener, 16)&lt;0) &#123; perror("listen"); return; &#125; FD_ZERO(&amp;readset); FD_ZERO(&amp;writeset); FD_ZERO(&amp;exset); while (1) &#123; maxfd = listener; FD_ZERO(&amp;readset); FD_ZERO(&amp;writeset); FD_ZERO(&amp;exset); FD_SET(listener, &amp;readset); for (i=0; i &lt; FD_SETSIZE; ++i) &#123; if (state[i]) &#123; if (i &gt; maxfd) maxfd = i; FD_SET(i, &amp;readset); if (state[i]-&gt;writing) &#123; FD_SET(i, &amp;writeset); &#125; &#125; &#125; if (select(maxfd+1, &amp;readset, &amp;writeset, &amp;exset, NULL) &lt; 0) &#123; perror("select"); return; &#125; if (FD_ISSET(listener, &amp;readset)) &#123; struct sockaddr_storage ss; socklen_t slen = sizeof(ss); int fd = accept(listener, (struct sockaddr*)&amp;ss, &amp;slen); if (fd &lt; 0) &#123; perror("accept"); &#125; else if (fd &gt; FD_SETSIZE) &#123; close(fd); &#125; else &#123; make_nonblocking(fd); state[fd] = alloc_fd_state(); assert(state[fd]);/*XXX*/ &#125; &#125; for (i=0; i &lt; maxfd+1; ++i) &#123; int r = 0; if (i == listener) continue; if (FD_ISSET(i, &amp;readset)) &#123; r = do_read(i, state[i]); &#125; if (r == 0 &amp;&amp; FD_ISSET(i, &amp;writeset)) &#123; // 如果没有读操作的时候才查看是否有写操作 r = do_write(i, state[i]); &#125; if (r) &#123; free_fd_state(state[i]); state[i] = NULL; close(i); &#125; &#125; &#125;&#125;intmain(int c, char **v)&#123; setvbuf(stdout, NULL, _IONBF, 0); run(); return 0;&#125; But we’re still not done. Because generating and reading the select() bit arrays takes time proportional to the largest fd that you provided for select(), the select() call scales terribly when the number of sockets is high.[On the userspace side, generating and reading the bit arrays can be made to take time proportional to the number of fds that you provided for select(). But on the kernel side, reading the bit arrays takes time proportional to the largest fd in the bit array, which tends to be around the total number of fds in use in the whole program, regardless of how many fds are added to the sets in select().] 但是我们依然还是没有完成。因为生成和读取 select() 的位数组的时间与你为select() 提供的最大 fd 成正比，当 sockets 数量非常多的时候，select() 函数调用的规模也就变得非常大。(因为 select() 函数就是一个轮询操作)对用户空间而言，生成和读取位数组的时间与你为 select() 供的fds数量成正比。但对内核空间而言，读取位数组所花费的时间与位数组中最大的 fd 成正比。无论在select() 将多少个 fds 添加到集合中，这往往是整个程序中使用的 fds 总数的一半。 Different operating systems have provided different replacement functions for select. These include poll(), epoll(), kqueue(), evports, and /dev/poll. All of these give better performance than select(), and all but poll() give O(1) performance for adding a socket, removing a socket, and for noticing that a socket is ready for IO. 不用的操作系统会提供取代 select 的其他函数。其中包括 poll(), epoll(), kqueue(), evports, and /dev/poll。这里所有的函数表现出来的性能都比 select() 要好，而且除了 poll() 之外，其他所有函数用于添加 socket，删除 socket 以及通知(socket 已准备好用于IO)操作都可以提供 O(1) 的性能。 Unfortunately, none of the efficient interfaces is a ubiquitous standard. Linux has epoll(), the BSDs (including Darwin) have kqueue(), Solaris has evports and /dev/poll… and none of these operating systems has any of the others. So if you want to write a portable high-performance asynchronous application, you’ll need an abstraction that wraps all of these interfaces, and provides whichever one of them is the most efficient. 不幸的是，这些高效的接口并没有普及成标准。Linux 用的是 epoll(), 在 BSD(包括 Darwin)用的是 kqueue()，Solaris 用的是 evports 和 /dev/poll… 这些操作系统各自为政。所以，如果你想写出一个轻量且搞性能的异步应用的话，你需要抽象的封装着所有接口，并且为接口使用者选择最有效的一个。 And that’s what the lowest level of the Libevent API does for you. It provides a consistent interface to various select() replacements, using the most efficient version available on the computer where it’s running. 这就是Libevent API的底层所做的事情，它对不同种类的 select() 替代函数提供了一致的接口，在计算机运行当中使用了最高效可用的版本。 Here’s yet another version of our asynchronous ROT13 server. This time, it uses Libevent 2 instead of select(). Note that the fd_sets are gone now: instead, we associate and disassociate events with a struct event_base, which might be implemented in terms of select(), poll(), epoll(), kqueue(), etc. 这是我们的异步ROT13服务器的另一个版本。这次，它使用Libevent 2而不是select()，注意，现在fd_sets已经消失：相反，我们可以将事件与结构event_base关联和解除关联, 这是用select(), poll(), epoll(), kqueue() 等方式实现的。 Example: A low-level ROT13 server with Libevent123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210/* For sockaddr_in */#include &lt;netinet/in.h&gt;/* For socket functions */#include &lt;sys/socket.h&gt;/* For fcntl */#include &lt;fcntl.h&gt;#include &lt;event2/event.h&gt;#include &lt;assert.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#define MAX_LINE 16384void do_read(evutil_socket_t fd, short events, void *arg);void do_write(evutil_socket_t fd, short events, void *arg);charrot13_char(char c)&#123; /* We don't want to use isalpha here; setting the locale would change * which characters are considered alphabetical. */ if ((c &gt;= 'a' &amp;&amp; c &lt;= 'm') || (c &gt;= 'A' &amp;&amp; c &lt;= 'M')) return c + 13; else if ((c &gt;= 'n' &amp;&amp; c &lt;= 'z') || (c &gt;= 'N' &amp;&amp; c &lt;= 'Z')) return c - 13; else return c;&#125;struct fd_state &#123; char buffer[MAX_LINE]; size_t buffer_used; size_t n_written; size_t write_upto; struct event *read_event; struct event *write_event;&#125;;struct fd_state *alloc_fd_state(struct event_base *base, evutil_socket_t fd)&#123; struct fd_state *state = malloc(sizeof(struct fd_state)); if (!state) return NULL; state-&gt;read_event = event_new(base, fd, EV_READ|EV_PERSIST, do_read, state); if (!state-&gt;read_event) &#123; free(state); return NULL; &#125; state-&gt;write_event = event_new(base, fd, EV_WRITE|EV_PERSIST, do_write, state); if (!state-&gt;write_event) &#123; event_free(state-&gt;read_event); free(state); return NULL; &#125; state-&gt;buffer_used = state-&gt;n_written = state-&gt;write_upto = 0; assert(state-&gt;write_event); return state;&#125;voidfree_fd_state(struct fd_state *state)&#123; event_free(state-&gt;read_event); event_free(state-&gt;write_event); free(state);&#125;voiddo_read(evutil_socket_t fd, short events, void *arg)&#123; struct fd_state *state = arg; char buf[1024]; int i; ssize_t result; while (1) &#123; assert(state-&gt;write_event); result = recv(fd, buf, sizeof(buf), 0); if (result &lt;= 0) break; for (i=0; i &lt; result; ++i) &#123; if (state-&gt;buffer_used &lt; sizeof(state-&gt;buffer)) state-&gt;buffer[state-&gt;buffer_used++] = rot13_char(buf[i]); if (buf[i] == '\n') &#123; assert(state-&gt;write_event); event_add(state-&gt;write_event, NULL); state-&gt;write_upto = state-&gt;buffer_used; &#125; &#125; &#125; if (result == 0) &#123; free_fd_state(state); &#125; else if (result &lt; 0) &#123; if (errno == EAGAIN) // XXXX use evutil macro return; perror("recv"); free_fd_state(state); &#125;&#125;voiddo_write(evutil_socket_t fd, short events, void *arg)&#123; struct fd_state *state = arg; while (state-&gt;n_written &lt; state-&gt;write_upto) &#123; ssize_t result = send(fd, state-&gt;buffer + state-&gt;n_written, state-&gt;write_upto - state-&gt;n_written, 0); if (result &lt; 0) &#123; if (errno == EAGAIN) // XXX use evutil macro return; free_fd_state(state); return; &#125; assert(result != 0); state-&gt;n_written += result; &#125; if (state-&gt;n_written == state-&gt;buffer_used) state-&gt;n_written = state-&gt;write_upto = state-&gt;buffer_used = 1; event_del(state-&gt;write_event);&#125;voiddo_accept(evutil_socket_t listener, short event, void *arg)&#123; struct event_base *base = arg; struct sockaddr_storage ss; socklen_t slen = sizeof(ss); int fd = accept(listener, (struct sockaddr*)&amp;ss, &amp;slen); if (fd &lt; 0) &#123; // XXXX eagain?? perror("accept"); &#125; else if (fd &gt; FD_SETSIZE) &#123; close(fd); // XXX replace all closes with EVUTIL_CLOSESOCKET */ &#125; else &#123; struct fd_state *state; evutil_make_socket_nonblocking(fd); state = alloc_fd_state(base, fd); assert(state); /*XXX err*/ assert(state-&gt;write_event); event_add(state-&gt;read_event, NULL); &#125;&#125;voidrun(void)&#123; evutil_socket_t listener; struct sockaddr_in sin; struct event_base *base; struct event *listener_event; base = event_base_new(); if (!base) return; /*XXXerr*/ sin.sin_family = AF_INET; sin.sin_addr.s_addr = 0; sin.sin_port = htons(40713); listener = socket(AF_INET, SOCK_STREAM, 0); evutil_make_socket_nonblocking(listener);#ifndef WIN32 &#123; int one = 1; setsockopt(listener, SOL_SOCKET, SO_REUSEADDR, &amp;one, sizeof(one)); &#125;#endif if (bind(listener, (struct sockaddr*)&amp;sin, sizeof(sin)) &lt; 0) &#123; perror("bind"); return; &#125; if (listen(listener, 16)&lt;0) &#123; perror("listen"); return; &#125; listener_event = event_new(base, listener, EV_READ|EV_PERSIST, do_accept, (void*)base); /*XXX check it */ event_add(listener_event, NULL); event_base_dispatch(base);&#125;intmain(int c, char **v)&#123; setvbuf(stdout, NULL, _IONBF, 0); run(); return 0;&#125; (Other things to note in the code: instead of typing the sockets as “int”, we’re using the type evutil_socket_t. Instead of calling fcntl(O_NONBLOCK) to make the sockets nonblocking, we’re calling evutil_make_socket_nonblocking. These changes make our code compatible with the divergent parts of the Win32 networking API.) 代码中值得注意的是：sockets 并不是用 int 代表，而是用 evutil_socket_t 类型。并不是调用 fcntl(O_NONBLOCK) 让 sockets 变成非阻塞，而是调用 evutil_make_socket_nonblocking。这些改变让我们的代码兼容了 Win32 网络 API 的不同部分。 What about convenience? (and what about Windows?)You’ve probably noticed that as our code has gotten more efficient, it has also gotten more complex. Back when we were forking, we didn’t have to manage a buffer for each connection: we just had a separate stack-allocated buffer for each process. We didn’t need to explicitly track whether each socket was reading or writing: that was implicit in our location in the code. And we didn’t need a structure to track how much of each operation had completed: we just used loops and stack variables. 你可能注意到，当我们的代码变得更高效的同时，也变得更加复杂了。回到我们使用 fork 的时候，我们并不需要为每个连接管理一个缓冲区。我们仅仅为每个进程分配了一个单独的栈分配缓冲区。我们不需要明确地追中每个 socket 是读还是写：这在我们的代码中是隐含的。(child 函数每个进程自己管理 socket 读写)。我们不需要一个结构来跟踪每个操作已完成多少：我们只是使用循环和栈变量。 Moreover, if you’re deeply experienced with networking on Windows, you’ll realize that Libevent probably isn’t getting optimal performance when it’s used as in the example above. On Windows, the way you do fast asynchronous IO is not with a select()-like interface: it’s by using the IOCP (IO Completion Ports) API. Unlike all the fast networking APIs, IOCP does not alert your program when a socket is ready for an operation that your program then has to perform. Instead, the program tells the Windows networking stack to start a network operation, and IOCP tells the program when the operation has finished. 此外，如果你对 Windows 的网络开发有深入了解，你将会意识到用在上面的例子当中 libevent 可能没有表现出最佳性能。在Windows上，快速异步IO的方式不是使用类 select() 接口：而是通过使用IOCP（IO Completion Ports）API。跟其他快速网络 API 不同，当 socket 准备好执行您的程序必须执行的操作时，IOCP不会通知你的程序。相反，程序会通知 Windows 网络栈启动网络操作，IOCP 会在操作完成时告诉程序。 Fortunately, the Libevent 2 “bufferevents” interface solves both of these issues: it makes programs much simpler to write, and provides an interface that Libevent can implement efficiently on Windows and on Unix. 幸运的是，Libevent 2 的 “bufferevents” 接口解决了这两个问题：它使程序编写起来更加简单，并提供了在 Windows 和 Unix 上高效实现的接口。 Here’s our ROT13 server one last time, using the bufferevents API. 这是我们 ROT13 服务器最后一次编码了，使用 bufferevents API。 Example: A simpler ROT13 server with Libevent123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/* For sockaddr_in */#include &lt;netinet/in.h&gt;/* For socket functions */#include &lt;sys/socket.h&gt;/* For fcntl */#include &lt;fcntl.h&gt;#include &lt;event2/event.h&gt;#include &lt;event2/buffer.h&gt;#include &lt;event2/bufferevent.h&gt;#include &lt;assert.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;errno.h&gt;#define MAX_LINE 16384void do_read(evutil_socket_t fd, short events, void *arg);void do_write(evutil_socket_t fd, short events, void *arg);charrot13_char(char c)&#123; /* We don't want to use isalpha here; setting the locale would change * which characters are considered alphabetical. */ if ((c &gt;= 'a' &amp;&amp; c &lt;= 'm') || (c &gt;= 'A' &amp;&amp; c &lt;= 'M')) return c + 13; else if ((c &gt;= 'n' &amp;&amp; c &lt;= 'z') || (c &gt;= 'N' &amp;&amp; c &lt;= 'Z')) return c - 13; else return c;&#125;voidreadcb(struct bufferevent *bev, void *ctx)&#123; struct evbuffer *input, *output; char *line; size_t n; int i; input = bufferevent_get_input(bev); output = bufferevent_get_output(bev); while ((line = evbuffer_readln(input, &amp;n, EVBUFFER_EOL_LF))) &#123; for (i = 0; i &lt; n; ++i) line[i] = rot13_char(line[i]); evbuffer_add(output, line, n); evbuffer_add(output, "\n", 1); free(line); &#125; if (evbuffer_get_length(input) &gt;= MAX_LINE) &#123; /* Too long; just process what there is and go on so that the buffer * doesn't grow infinitely long. */ char buf[1024]; while (evbuffer_get_length(input)) &#123; int n = evbuffer_remove(input, buf, sizeof(buf)); for (i = 0; i &lt; n; ++i) buf[i] = rot13_char(buf[i]); evbuffer_add(output, buf, n); &#125; evbuffer_add(output, "\n", 1); &#125;&#125;voiderrorcb(struct bufferevent *bev, short error, void *ctx)&#123; if (error &amp; BEV_EVENT_EOF) &#123; /* connection has been closed, do any clean up here */ /* ... */ &#125; else if (error &amp; BEV_EVENT_ERROR) &#123; /* check errno to see what error occurred */ /* ... */ &#125; else if (error &amp; BEV_EVENT_TIMEOUT) &#123; /* must be a timeout event handle, handle it */ /* ... */ &#125; bufferevent_free(bev);&#125;voiddo_accept(evutil_socket_t listener, short event, void *arg)&#123; struct event_base *base = arg; struct sockaddr_storage ss; socklen_t slen = sizeof(ss); int fd = accept(listener, (struct sockaddr*)&amp;ss, &amp;slen); if (fd &lt; 0) &#123; perror("accept"); &#125; else if (fd &gt; FD_SETSIZE) &#123; close(fd); &#125; else &#123; struct bufferevent *bev; evutil_make_socket_nonblocking(fd); bev = bufferevent_socket_new(base, fd, BEV_OPT_CLOSE_ON_FREE); bufferevent_setcb(bev, readcb, NULL, errorcb, NULL); bufferevent_setwatermark(bev, EV_READ, 0, MAX_LINE); bufferevent_enable(bev, EV_READ|EV_WRITE); &#125;&#125;voidrun(void)&#123; evutil_socket_t listener; struct sockaddr_in sin; struct event_base *base; struct event *listener_event; base = event_base_new(); if (!base) return; /*XXXerr*/ sin.sin_family = AF_INET; sin.sin_addr.s_addr = 0; sin.sin_port = htons(40713); listener = socket(AF_INET, SOCK_STREAM, 0); evutil_make_socket_nonblocking(listener);#ifndef WIN32 &#123; int one = 1; setsockopt(listener, SOL_SOCKET, SO_REUSEADDR, &amp;one, sizeof(one)); &#125;#endif if (bind(listener, (struct sockaddr*)&amp;sin, sizeof(sin)) &lt; 0) &#123; perror("bind"); return; &#125; if (listen(listener, 16)&lt;0) &#123; perror("listen"); return; &#125; listener_event = event_new(base, listener, EV_READ|EV_PERSIST, do_accept, (void*)base); /*XXX check it */ event_add(listener_event, NULL); event_base_dispatch(base);&#125;intmain(int c, char **v)&#123; setvbuf(stdout, NULL, _IONBF, 0); run(); return 0;&#125;]]></content>
      <tags>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RAC 源码分析(一)]]></title>
    <url>%2F2018%2F04%2F02%2Frac-source-analyze%2F</url>
    <content type="text"><![CDATA[RecativeCoCoa objectivec 版本的分析，第一部分主要阐述 rac_signalForSelector: 该方法的实现，观摩一下 RAC 是如何监听方法回调的. SEL aliasSelector = RACAliasForSelector(selector); 这个方法得到了字符串拼接 rac_alias_ + @selector 后的方法 aliasSelector 用于后面替换原方法，实现方法监听。 RACSubject *subject 为热信号，检测是否已经在监听改方法，如果有，把信号返回。 RACSwizzleClass) 动态创建好这个类的 RAC 关联类，为 class + _RACSelectorSignal 并把这个映射类的符号添加到 OC 动态类符号当中，然后将 RAC 关联类中的方法转发 forwardInvocation: 、方法响应 respondsToSelector: 创建热信号 RACSubject 先查看一下对象 object 是否存在被监听的实例方法，如果不存在而查看被监听方法是否一个协议方法. class_replaceMethod 方法把参数 selector 方法替换成 OC消息转发方法_objc_msgForward，这样的话所有被监听的方法都会调用 forwardInvocation: 方法了结合NSObject文档可以知道，_objc_msgForward 消息转发做了如下几件事： 1.调用resolveInstanceMethod:方法，允许用户在此时为该Class动态添加实现。如果有实现了，则调用并返回。如果仍没实现，继续下面的动作。2.调用forwardingTargetForSelector:方法，尝试找到一个能响应该消息的对象。如果获取到，则直接转发给它。如果返回了nil，继续下面的动作。3.调用methodSignatureForSelector:方法，尝试获得一个方法签名。如果获取不到，则直接调用doesNotRecognizeSelector抛出异常。4.调用forwardInvocation:方法，将地3步获取到的方法签名包装成Invocation传入，如何处理就在这里面了。 static Class RACSwizzleClass(NSObject *self)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static Class RACSwizzleClass(NSObject *self) &#123; // 1. Class statedClass = self.class; Class baseClass = object_getClass(self); Class knownDynamicSubclass = objc_getAssociatedObject(self, RACSubclassAssociationKey); if (knownDynamicSubclass != Nil) return knownDynamicSubclass; NSString *className = NSStringFromClass(baseClass); if (statedClass != baseClass) &#123; @synchronized (swizzledClasses()) &#123; if (![swizzledClasses() containsObject:className]) &#123; RACSwizzleForwardInvocation(baseClass); RACSwizzleRespondsToSelector(baseClass); RACSwizzleGetClass(baseClass, statedClass); RACSwizzleGetClass(object_getClass(baseClass), statedClass); RACSwizzleMethodSignatureForSelector(baseClass); [swizzledClasses() addObject:className]; &#125; &#125; return baseClass; &#125; const char *subclassName = [className stringByAppendingString:RACSubclassSuffix].UTF8String; Class subclass = objc_getClass(subclassName); if (subclass == nil) &#123; subclass = [RACObjCRuntime createClass:subclassName inheritingFromClass:baseClass]; if (subclass == nil) return nil; RACSwizzleForwardInvocation(subclass); RACSwizzleRespondsToSelector(subclass); RACSwizzleGetClass(subclass, statedClass); RACSwizzleGetClass(object_getClass(subclass), statedClass); RACSwizzleMethodSignatureForSelector(subclass); objc_registerClassPair(subclass); &#125; object_setClass(self, subclass); objc_setAssociatedObject(self, RACSubclassAssociationKey, subclass, OBJC_ASSOCIATION_ASSIGN); return subclass;&#125; object.class 由于KVO重写了class 方法，所以不能准确的找到类.object_getClass() 方法可以准确的找到 isa 指针.object.class 与 object_getClass(object) 进行判断 来防止KVO导致的AOP无效.为什么会发生这种情况？因为当你使用 KVO 的时候，系统会帮你重写类的 class 方法，生成 NSKVONotifying_ + Class。例如如下代码12345678[self.view addObserver:self forKeyPath:@&quot;frame&quot; options:NSKeyValueObservingOptionNew context:nil];- (void)observeValueForKeyPath:(nullable NSString *)keyPath ofObject:(nullable id)object change:(nullable NSDictionary&lt;NSKeyValueChangeKey, id&gt; *)change context:(nullable void *)context &#123; Class statedClass = self.view.class; Class baseClass = object_getClass(self.view); NSLog(@&quot;%@ %@&quot;, NSStringFromClass(statedClass), NSStringFromClass(baseClass));&#125; 得到的 log 如下：RACLearning[3657:478602] UIView NSKVONotifying_UIView swizzledClasses() 专门用来存储 isa 被修改过的类，这些类不用重新生成为 RAC 的类. 重新生成 RAC 的类 Class + _RACSelectorSignal，如果类符号中不存在，则利用运行时粗行家改类符号，并继承 Class。 RACSwizzleForwardInvocation) 利用运行时替换消息转发的方法forwardInvocation: ，由 NSObjectRACSignalForSelector) 代码中可以看到，所传入的方法参数 selector 都被换成了 _objc_msgForward 方法，所以，当方法 selector 调用是，自然就会调用 forwardInvocation: 方法了。 static void RACSwizzleForwardInvocation(Class class)1234567891011121314151617181920212223static void RACSwizzleForwardInvocation(Class class) &#123; SEL forwardInvocationSEL = @selector(forwardInvocation:); Method forwardInvocationMethod = class_getInstanceMethod(class, forwardInvocationSEL); // Preserve any existing implementation of -forwardInvocation:. void (*originalForwardInvocation)(id, SEL, NSInvocation *) = NULL; if (forwardInvocationMethod != NULL) &#123; originalForwardInvocation = (__typeof__(originalForwardInvocation))method_getImplementation(forwardInvocationMethod); &#125; id newForwardInvocation = ^(id self, NSInvocation *invocation) &#123; BOOL matched = RACForwardInvocation(self, invocation); if (matched) return; if (originalForwardInvocation == NULL) &#123; [self doesNotRecognizeSelector:invocation.selector]; &#125; else &#123; originalForwardInvocation(self, forwardInvocationSEL, invocation); &#125; &#125;; class_replaceMethod(class, forwardInvocationSEL, imp_implementationWithBlock(newForwardInvocation), "v@:@");&#125; 定义 IMP 函数指针指针 originalForwardInvocation ，指向 class 类的 forwardInvocation 内部函数实现， 然后创建新的 forwardInvocation: block 函数 newForwardInvocation, block 函数内部先调用 RACForwardInvocation) 函数，以调用映射过后的 rac_alias_ + @selector 函数(实际上是 @selector的复制体)，然后给热信号 subject发送信号 Next 以调用订阅 block nexblock. 利用 class_replaceMethod 方法替换了 class 的对象方法 forwardInvocation: 为新的 newForwardInvocation block 函数. static BOOL RACForwardInvocation(id self, NSInvocation *invocation)12345678910111213141516static BOOL RACForwardInvocation(id self, NSInvocation *invocation) &#123; SEL aliasSelector = RACAliasForSelector(invocation.selector); RACSubject *subject = objc_getAssociatedObject(self, aliasSelector); Class class = object_getClass(invocation.target); BOOL respondsToAlias = [class instancesRespondToSelector:aliasSelector]; if (respondsToAlias) &#123; invocation.selector = aliasSelector; [invocation invoke]; &#125; if (subject == nil) return respondsToAlias; [subject sendNext:invocation.rac_argumentsTuple]; return YES;&#125; 由 NSObjectRACSignalForSelector) 中知道，热信号 subject 已经被创建了， 而且利用了 class_addMethod 方法完成了被监听方法 @selector 的复制体 rac_alias_ + @selector 方法，所以这里直接利用 [invocation invoke]调用 rac_alias_ + @selector 方法，然后再像热信号 subject 发送 next 信号并且带上 rac_alias_ + @selector 方法的参数数组rac_argumentsTuple以调用 nextblock. - (RACTuple *)rac_argumentsTuple123456789- (RACTuple *)rac_argumentsTuple &#123; NSUInteger numberOfArguments = self.methodSignature.numberOfArguments; NSMutableArray *argumentsArray = [NSMutableArray arrayWithCapacity:numberOfArguments - 2]; for (NSUInteger index = 2; index &lt; numberOfArguments; index++) &#123; [argumentsArray addObject:[self rac_argumentAtIndex:index] ?: RACTupleNil.tupleNil]; &#125; return [RACTuple tupleWithObjectsFromArray:argumentsArray];&#125; 参数个数 methodSignature.numberOfArguments 默认有一个 _cmd 一个 target 所以要 -2 获取该方法的参数 ，rac_argumentAtIndex 函数内通过 methodSignature 的 getArgumentTypeAtIndex 方法来判断获取 OC 对象或 基础类型(如 int，char，bool 等)转成的 NSNumber 对象。 ?: 新写法，如果有则添加返回的 OC 对象，没有就把 RACTupleNil.tupleNil 单例对象添加进去（为什么添加单例对象？可以节省内存！） 利用 RACTuple 对象吧参数数组包装一下返回。i]]></content>
      <tags>
        <tag>源码分析</tag>
      </tags>
  </entry>
</search>
